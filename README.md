# Efficient Transformer

## Introduction

 

## Pruning

### depth

> 1. Fan, A., Grave, E. and Joulin, A., 2019. Reducing transformer depth on demand with structured dropout. *arXiv preprint arXiv:1909.11556*.

#### motivation



#### methods



#### experiments & results



> 2. Hou, L., Huang, Z., Shang, L., Jiang, X., Chen, X. and Liu, Q., 2020. Dynabert: Dynamic bert with adaptive width and depth. *Advances in Neural Information Processing Systems*, *33*, pp.9782-9793.



### width

> 1. Michel, P., Levy, O. and Neubig, G., 2019. Are sixteen heads really better than one?. *Advances in neural information processing systems*, *32*.



> 2. Prasanna, S., Rogers, A. and Rumshisky, A., 2020. When bert plays the lottery, all tickets are winning. *arXiv preprint arXiv:2005.00561*.



> 3. Zhu, M., Tang, Y. and Han, K., 2021. Vision transformer pruning. *arXiv preprint arXiv:2104.08500*.





## Decomposition



## Quantization



## Knowledge Distillation



## Compact Architecture



