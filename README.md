# awesome-efficient-transformer

## Introduction





 ## Papers

### Surveys

2022 | IEEE | Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu, C., Xu, Y. and Yang, Z., 2022. A survey on vision transformer. *IEEE transactions on pattern analysis and machine intelligence*.

2020 | ACM | Tay, Y., Dehghani, M., Bahri, D. and Metzler, D., 2020. Efficient transformers: A survey. *ACM Computing Surveys (CSUR)*.

### Pruning

#### depth

2020 | NIPS | Hou, L., Huang, Z., Shang, L., Jiang, X., Chen, X. and Liu, Q., 2020. Dynabert: Dynamic bert with adaptive width and depth. *Advances in Neural Information Processing Systems*, *33*, pp.9782-9793.

2020 | ICLR | Fan, A., Grave, E. and Joulin, A., 2019. Reducing transformer depth on demand with structured dropout. *arXiv preprint arXiv:1909.11556*.

#### width

2021 | KDD | Zhu, M., Tang, Y. and Han, K., 2021. Vision transformer pruning. *arXiv preprint arXiv:2104.08500*.

2020 | EMNLP | Prasanna, S., Rogers, A. and Rumshisky, A., 2020. When bert plays the lottery, all tickets are winning. *arXiv preprint arXiv:2005.00561*.

2019 | NIPS | Michel, P., Levy, O. and Neubig, G., 2019. Are sixteen heads really better than one?. *Advances in neural information processing systems*, *32*.



### Quantization



### Decomposition

2020 | EMNLP | Wang, Z., Wohlwend, J. and Lei, T., 2019. Structured pruning of large language models. *arXiv preprint arXiv:1910.04732*.



### Knowledge Distillation



### Architecture Design





## Project



#### 

